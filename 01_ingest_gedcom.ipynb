{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db33bc43-69be-4860-8210-f6c877aec20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STAGING_DIR = \"/Volumes/workspace/staging_google_drive/gedcom/Ancestry GEDCOM Exports/\"\n",
    "\n",
    "files = dbutils.fs.ls(STAGING_DIR)\n",
    "print(files)\n",
    "\n",
    "latest = spark.table(\"genealogy.control_latest_gedcom\").collect()[0]\n",
    "latest_filename = latest['source_filename']\n",
    "latest_file_id = latest['file_id']\n",
    "latest_size = latest['file_size']\n",
    "latest_sync_time = latest['_fivetran_synced']\n",
    "latest_path = f\"{STAGING_DIR}{latest_filename}\"\n",
    "\n",
    "latest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37971d12-1c37-4f95-8ae4-9ac21a2f2a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# idempotency check on exact file id\n",
    "\n",
    "already_ingested = (\n",
    "    spark.table(\"genealogy.bronze_gedcom\")\n",
    "    .filter(f\"source_file_id = '{latest_file_id}'\")\n",
    "    .limit(1)\n",
    "    .count()\n",
    "    > 0\n",
    ")\n",
    "\n",
    "if already_ingested:\n",
    "    raise RuntimeError(\n",
    "        f\"SKIP_PIPELINE: GEDCOM file {latest_file_id} has already been ingested\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6577a5cd-7afe-4ed0-a77b-7949938704b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CANONICAL_DIR = \"/Volumes/workspace/genealogy/file_uploads/\"\n",
    "CANONICAL_FILE = f\"{CANONICAL_DIR}/Genealogy of Ed Ball.ged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1587d50d-71dd-4e04-b7a2-5a0f115f629a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure destination directory exists\n",
    "dbutils.fs.mkdirs(CANONICAL_DIR)\n",
    "\n",
    "# Remove existing canonical file if present\n",
    "try:\n",
    "    dbutils.fs.rm(CANONICAL_FILE)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Copy latest snapshot into canonical location\n",
    "dbutils.fs.cp(latest_path, CANONICAL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3519ffe5-7c5d-499c-9755-4f42a8003e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GEDCOM_PATH = CANONICAL_FILE\n",
    "\n",
    "rows = []\n",
    "\n",
    "with open(GEDCOM_PATH.replace(\"file:\", \"\"), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        parts = line.split(\" \", 2)\n",
    "\n",
    "        level = int(parts[0])\n",
    "        xref_id = None\n",
    "        tag = None\n",
    "        value = None\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            tag = parts[1]\n",
    "        elif len(parts) == 3:\n",
    "            if parts[1].startswith(\"@\") and parts[1].endswith(\"@\"):\n",
    "                xref_id = parts[1]\n",
    "                tag = parts[2]\n",
    "            else:\n",
    "                tag = parts[1]\n",
    "                value = parts[2]\n",
    "\n",
    "        rows.append((i, level, xref_id, tag, value))\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    rows,\n",
    "    [\"line_no\", \"level\", \"xref_id\", \"tag\", \"value\"]\n",
    ")\n",
    "\n",
    "display(df.limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7f0b11-f308-4f41-9b11-b57772bcd6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# guardrails for new file id, but same content or anomalous content\n",
    "\n",
    "from pyspark.sql.functions import sha2, concat_ws, collect_list\n",
    "\n",
    "new_record_count = df.count()\n",
    "\n",
    "content_hash = (\n",
    "    df\n",
    "    .select(sha2(concat_ws(\"||\", *df.columns), 256).alias(\"row_hash\"))\n",
    "    .agg(sha2(concat_ws(\"||\", collect_list(\"row_hash\")), 256).alias(\"hash\"))\n",
    "    .collect()[0][\"hash\"]\n",
    ")\n",
    "\n",
    "prev = (\n",
    "    spark.table(\"genealogy.control_ingested_gedcom\")\n",
    "    .orderBy(\"ingested_at\", ascending=False)\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "prev = prev[0] if prev else None\n",
    "\n",
    "checks = []\n",
    "\n",
    "if prev:\n",
    "    size_ratio = latest_size / prev[\"size\"] if prev[\"size\"] else 1\n",
    "    record_ratio = new_record_count / prev[\"record_count\"] if prev[\"record_count\"] else 1\n",
    "\n",
    "    checks.append((\"size_change\", size_ratio))\n",
    "    checks.append((\"record_change\", record_ratio))\n",
    "    checks.append((\"hash_changed\", content_hash != prev[\"hash\"]))\n",
    "\n",
    "if prev:\n",
    "    if (\n",
    "        abs(size_ratio - 1) < 0.01\n",
    "        and abs(record_ratio - 1) < 0.01\n",
    "        and content_hash == prev[\"hash\"]\n",
    "    ):\n",
    "        raise RuntimeError(\n",
    "            \"INTENTIONAL_SKIP: GEDCOM identical to previous ingestion\"\n",
    "        )\n",
    "\n",
    "    if record_ratio < 0.5:\n",
    "        raise RuntimeError(\n",
    "            \"BLOCK_PIPELINE: GEDCOM record count dropped >50% (likely broken export)\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c573ba1a-51ff-459e-9f46-943a2b63858f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "source_file = latest[\"source_path\"]\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"source_file\", lit(latest_filename))\n",
    "    .withColumn(\"source_file_id\", lit(latest_file_id))\n",
    "    .withColumn(\"source_file_synced_at\", lit(latest_sync_time))\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"genealogy.bronze_gedcom\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb22d5f-4e0d-42e5-b34a-e25d0f2f0766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark.createDataFrame(\n",
    "    [(latest_file_id, latest_path, latest_size, new_record_count, content_hash)],\n",
    "    [\"file_id\", \"source_path\", \"size\", \"record_count\", \"hash\"]\n",
    ").withColumn(\"ingested_at\", current_timestamp()) \\\n",
    " .write.mode(\"append\") \\\n",
    " .saveAsTable(\"genealogy.control_ingested_gedcom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2553e6-dc39-44fb-b915-8a629d7b5915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM genealogy.bronze_gedcom;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce4abd5-0e31-4da1-8678-90c636f05435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW genealogy.bronze_gedcom_with_record AS\n",
    "SELECT\n",
    "  *,\n",
    "  last_value(\n",
    "    xref_id,\n",
    "    true\n",
    "  ) OVER (\n",
    "    ORDER BY line_no\n",
    "    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "  ) AS record_xref\n",
    "FROM genealogy.bronze_gedcom;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb115db-0b46-48af-9a03-9039bdade3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  record_xref,\n",
    "  COUNT(*) AS lines\n",
    "FROM genealogy.bronze_gedcom_with_record\n",
    "GROUP BY record_xref\n",
    "ORDER BY lines DESC\n",
    "LIMIT 10;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6010952177949360,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_gedcom",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
